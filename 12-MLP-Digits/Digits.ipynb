{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "yPwOS9zpBwzO"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.model_selection import train_test_split\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset=load_digits()\n",
        "X = dataset.data\n",
        "Y = dataset.target\n",
        "Y = np.eye(10)[Y]\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)\n",
        "X_train.shape, X_test.shape, Y_train.shape, Y_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HzeIZejQKQEZ",
        "outputId": "9f7408ab-991a-4646-87c9-1df642db12eb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1437, 64), (360, 64), (1437, 10), (360, 10))"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(X):\n",
        "  return 1 / (1 + np.exp(-X))\n",
        "\n",
        "def softmax(X):\n",
        "  return np.exp(X) / np.sum(np.exp(X))\n",
        "\n",
        "def root_mean_squaired_error(Y_get, Y_pred):\n",
        "  return np.sqrt(np.mean((Y_get - Y_pred) ** 2))"
      ],
      "metadata": {
        "id": "SivcEYdmSU9w"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 80\n",
        "lr = 0.001\n",
        "\n",
        "D_in = X_train.shape[1]\n",
        "H1 = 128\n",
        "H2 = 32\n",
        "D_out= Y_train.shape[1]\n"
      ],
      "metadata": {
        "id": "2PLGM897XlOr"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "W1 = np.random.randn(D_in , H1)\n",
        "W2 = np.random.randn(H1 , H2)\n",
        "W3 = np.random.randn(H2 , D_out)"
      ],
      "metadata": {
        "id": "MDsjQZt3ZloB"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "B1 = np.random.randn(1, H1)\n",
        "B2 = np.random.randn(1, H2)\n",
        "B3 = np.random.randn(1, D_out)"
      ],
      "metadata": {
        "id": "8SmzGIJKZ-wf"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(epochs):\n",
        "\n",
        "  # train\n",
        "  Y_pred_train=[]\n",
        "  for x, y in zip(X_train, Y_train):\n",
        "\n",
        "    x = x.reshape(-1,1)\n",
        "\n",
        "    # forward\n",
        "    # layer1\n",
        "    out1 = sigmoid(x.T @ W1 + B1)\n",
        "\n",
        "    #layer2\n",
        "    out2 = sigmoid(out1 @ W2 + B2)\n",
        "\n",
        "    #layer3\n",
        "    out3 = softmax(out2 @ W3 + B3)\n",
        "    y_pred = out3\n",
        "    Y_pred_train.append(y_pred)\n",
        "\n",
        "    # loss = root_mean_squaired_error(y, y_pred)\n",
        "\n",
        "    # backward\n",
        "    # layer3\n",
        "    error = -2 * (y - y_pred)\n",
        "    grad_B3 = error\n",
        "\n",
        "    grad_W3 =  out2.T @ error\n",
        "\n",
        "    # layer2\n",
        "    error = error @ W3.T * out2 * (1 - out2)\n",
        "    grad_B2 = error\n",
        "    grad_W2 = out1.T @ error\n",
        "\n",
        "    # layer1\n",
        "    error = error @ W2.T * out1 * (1 - out1)\n",
        "    grad_B1 = error\n",
        "    grad_W1 = x @ error\n",
        "\n",
        "    # update\n",
        "    # layer1\n",
        "    W1 -= lr * grad_W1\n",
        "    B1 -= lr * grad_B1\n",
        "\n",
        "    # layer2\n",
        "    W2 -= lr * grad_W2\n",
        "    B2 -= lr * grad_B2\n",
        "\n",
        "    # layer3\n",
        "    W3 -= lr * grad_W3\n",
        "    B3 -= lr * grad_B3\n",
        "\n",
        "  # test\n",
        "  Y_pred_test=[]\n",
        "  for x, y in zip(X_test, Y_test):\n",
        "\n",
        "    x = x.reshape(-1,1)\n",
        "\n",
        "    # forward\n",
        "    # layer1\n",
        "    out1 = sigmoid(x.T @ W1 + B1)\n",
        "\n",
        "    #layer2\n",
        "    out2 = sigmoid(out1 @ W2 + B2)\n",
        "\n",
        "    #layer3\n",
        "    out3 = softmax(out2 @ W3 + B3)\n",
        "    y_pred = out3\n",
        "    Y_pred_test.append(y_pred)\n",
        "\n",
        "  Y_pred_train = np.array(Y_pred_train).reshape(-1, 10)\n",
        "  loss_train = root_mean_squaired_error(Y_train, Y_pred_train)\n",
        "  accuracy_train = np.sum(np.argmax(Y_train, axis=1) == np.argmax(Y_pred_train, axis=1)) / len(Y_train)\n",
        "  print(\"loss train:\", loss_train)\n",
        "  print(\"accuracy train:\", accuracy_train)\n",
        "\n",
        "  Y_pred_test = np.array(Y_pred_test).reshape(-1, 10)\n",
        "  loss_test = root_mean_squaired_error(Y_test, Y_pred_test)\n",
        "  accuracy_test = np.sum(np.argmax(Y_test, axis=1) == np.argmax(Y_pred_test, axis=1)) / len(Y_test)\n",
        "  print(\"loss test:\", loss_test)\n",
        "  print(\"accuracy test:\", accuracy_test)\n",
        "\n",
        "    # acc = ...\n",
        "    # loss = ...\n",
        "\n",
        "    # print(W1, B1, W2, B2, W3, B3)\n",
        "\n"
      ],
      "metadata": {
        "id": "p48cGAgcbnxs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0003fd91-3495-4e67-acb0-785fced75210"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss train: 0.3172738509787928\n",
            "accuracy train: 0.19206680584551147\n",
            "loss test: 0.2800751933596708\n",
            "accuracy test: 0.35\n",
            "loss train: 0.26945730297670606\n",
            "accuracy train: 0.43702157272094644\n",
            "loss test: 0.25939834200738027\n",
            "accuracy test: 0.4777777777777778\n",
            "loss train: 0.24684985768403203\n",
            "accuracy train: 0.5539318023660403\n",
            "loss test: 0.2436511511869931\n",
            "accuracy test: 0.5666666666666667\n",
            "loss train: 0.23058019912877295\n",
            "accuracy train: 0.6409185803757829\n",
            "loss test: 0.23246707989324392\n",
            "accuracy test: 0.6111111111111112\n",
            "loss train: 0.2177279295176711\n",
            "accuracy train: 0.6993736951983298\n",
            "loss test: 0.22398726977910247\n",
            "accuracy test: 0.6472222222222223\n",
            "loss train: 0.20696368215456015\n",
            "accuracy train: 0.7272094641614475\n",
            "loss test: 0.21684911189143685\n",
            "accuracy test: 0.6833333333333333\n",
            "loss train: 0.19727861259433774\n",
            "accuracy train: 0.7613082811412665\n",
            "loss test: 0.20939983338996998\n",
            "accuracy test: 0.7111111111111111\n",
            "loss train: 0.18821279313088\n",
            "accuracy train: 0.7835768963117606\n",
            "loss test: 0.20255398355764195\n",
            "accuracy test: 0.7388888888888889\n",
            "loss train: 0.17987005823533816\n",
            "accuracy train: 0.8016701461377871\n",
            "loss test: 0.19679119642840537\n",
            "accuracy test: 0.75\n",
            "loss train: 0.172401679958965\n",
            "accuracy train: 0.825330549756437\n",
            "loss test: 0.19220975623722228\n",
            "accuracy test: 0.7527777777777778\n",
            "loss train: 0.1655859386280483\n",
            "accuracy train: 0.8503827418232429\n",
            "loss test: 0.18816289465031194\n",
            "accuracy test: 0.7555555555555555\n",
            "loss train: 0.1593832557143834\n",
            "accuracy train: 0.8629088378566457\n",
            "loss test: 0.1847677904792612\n",
            "accuracy test: 0.775\n",
            "loss train: 0.15378451218699235\n",
            "accuracy train: 0.8782185107863605\n",
            "loss test: 0.18183332439466268\n",
            "accuracy test: 0.7833333333333333\n",
            "loss train: 0.14856977179984762\n",
            "accuracy train: 0.8858733472512178\n",
            "loss test: 0.1791818631723139\n",
            "accuracy test: 0.7888888888888889\n",
            "loss train: 0.14372502904354414\n",
            "accuracy train: 0.894919972164231\n",
            "loss test: 0.1767025088632029\n",
            "accuracy test: 0.7944444444444444\n",
            "loss train: 0.1391406432755138\n",
            "accuracy train: 0.9046624913013221\n",
            "loss test: 0.17438223953274276\n",
            "accuracy test: 0.7972222222222223\n",
            "loss train: 0.13481920619008075\n",
            "accuracy train: 0.9102296450939458\n",
            "loss test: 0.17228195754760292\n",
            "accuracy test: 0.8027777777777778\n",
            "loss train: 0.1308267159562321\n",
            "accuracy train: 0.9123173277661796\n",
            "loss test: 0.170367326399474\n",
            "accuracy test: 0.8055555555555556\n",
            "loss train: 0.12720768554779008\n",
            "accuracy train: 0.9171885873347251\n",
            "loss test: 0.1685669789454631\n",
            "accuracy test: 0.8111111111111111\n",
            "loss train: 0.12375200784517244\n",
            "accuracy train: 0.9213639526791928\n",
            "loss test: 0.166844800736565\n",
            "accuracy test: 0.8111111111111111\n",
            "loss train: 0.12050058014001795\n",
            "accuracy train: 0.9255393180236604\n",
            "loss test: 0.16523237319867742\n",
            "accuracy test: 0.8194444444444444\n",
            "loss train: 0.11743599759205547\n",
            "accuracy train: 0.9318023660403618\n",
            "loss test: 0.1637365509007241\n",
            "accuracy test: 0.8222222222222222\n",
            "loss train: 0.1145128895176842\n",
            "accuracy train: 0.9345859429366736\n",
            "loss test: 0.1623451236426028\n",
            "accuracy test: 0.8277777777777777\n",
            "loss train: 0.11175697051486115\n",
            "accuracy train: 0.9401530967292971\n",
            "loss test: 0.16098199415651043\n",
            "accuracy test: 0.8277777777777777\n",
            "loss train: 0.10915284841154191\n",
            "accuracy train: 0.941544885177453\n",
            "loss test: 0.15965890484182763\n",
            "accuracy test: 0.8277777777777777\n",
            "loss train: 0.10663031944355791\n",
            "accuracy train: 0.9450243562978428\n",
            "loss test: 0.15846063029842808\n",
            "accuracy test: 0.8333333333333334\n",
            "loss train: 0.1041748803109327\n",
            "accuracy train: 0.9464161447459986\n",
            "loss test: 0.15741132824407508\n",
            "accuracy test: 0.8305555555555556\n",
            "loss train: 0.10178397492109875\n",
            "accuracy train: 0.9491997216423104\n",
            "loss test: 0.15644523835679147\n",
            "accuracy test: 0.8416666666666667\n",
            "loss train: 0.09952621062738283\n",
            "accuracy train: 0.9498956158663883\n",
            "loss test: 0.15561706441020498\n",
            "accuracy test: 0.8444444444444444\n",
            "loss train: 0.09738185627523945\n",
            "accuracy train: 0.9512874043145442\n",
            "loss test: 0.15491048189919443\n",
            "accuracy test: 0.8444444444444444\n",
            "loss train: 0.09532527037234804\n",
            "accuracy train: 0.9561586638830898\n",
            "loss test: 0.15427329114896682\n",
            "accuracy test: 0.8444444444444444\n",
            "loss train: 0.0933452797270179\n",
            "accuracy train: 0.9596381350034795\n",
            "loss test: 0.15372436197043932\n",
            "accuracy test: 0.8416666666666667\n",
            "loss train: 0.09143914929511338\n",
            "accuracy train: 0.9610299234516354\n",
            "loss test: 0.15319950211612696\n",
            "accuracy test: 0.8416666666666667\n",
            "loss train: 0.08961725127044357\n",
            "accuracy train: 0.9624217118997912\n",
            "loss test: 0.15262097067840405\n",
            "accuracy test: 0.8388888888888889\n",
            "loss train: 0.08787158424881722\n",
            "accuracy train: 0.964509394572025\n",
            "loss test: 0.15207633007142937\n",
            "accuracy test: 0.8388888888888889\n",
            "loss train: 0.0862404243247207\n",
            "accuracy train: 0.9665970772442589\n",
            "loss test: 0.15158928700227928\n",
            "accuracy test: 0.8361111111111111\n",
            "loss train: 0.0847109435185212\n",
            "accuracy train: 0.9686847599164927\n",
            "loss test: 0.15114209942348839\n",
            "accuracy test: 0.8305555555555556\n",
            "loss train: 0.08325087827322349\n",
            "accuracy train: 0.9693806541405706\n",
            "loss test: 0.15073164539669848\n",
            "accuracy test: 0.8305555555555556\n",
            "loss train: 0.08187349324988928\n",
            "accuracy train: 0.9714683368128044\n",
            "loss test: 0.15034580628378458\n",
            "accuracy test: 0.8361111111111111\n",
            "loss train: 0.0805695186710145\n",
            "accuracy train: 0.9714683368128044\n",
            "loss test: 0.1499749924247034\n",
            "accuracy test: 0.8361111111111111\n",
            "loss train: 0.07931220597337947\n",
            "accuracy train: 0.9721642310368824\n",
            "loss test: 0.1496185302074508\n",
            "accuracy test: 0.8361111111111111\n",
            "loss train: 0.07810456536282163\n",
            "accuracy train: 0.9735560194850382\n",
            "loss test: 0.14927483668131908\n",
            "accuracy test: 0.8361111111111111\n",
            "loss train: 0.07696309907099376\n",
            "accuracy train: 0.9742519137091162\n",
            "loss test: 0.14893954308069093\n",
            "accuracy test: 0.8361111111111111\n",
            "loss train: 0.07587402618192234\n",
            "accuracy train: 0.9749478079331941\n",
            "loss test: 0.14860850770981157\n",
            "accuracy test: 0.8361111111111111\n",
            "loss train: 0.07481866223145626\n",
            "accuracy train: 0.9770354906054279\n",
            "loss test: 0.14827967599438396\n",
            "accuracy test: 0.8388888888888889\n",
            "loss train: 0.07378586902816194\n",
            "accuracy train: 0.9770354906054279\n",
            "loss test: 0.1479527167247735\n",
            "accuracy test: 0.8416666666666667\n",
            "loss train: 0.0727699618961812\n",
            "accuracy train: 0.977731384829506\n",
            "loss test: 0.14762728488670704\n",
            "accuracy test: 0.8416666666666667\n",
            "loss train: 0.07176720924706974\n",
            "accuracy train: 0.9791231732776617\n",
            "loss test: 0.1473028211652663\n",
            "accuracy test: 0.8472222222222222\n",
            "loss train: 0.07078009930177646\n",
            "accuracy train: 0.9798190675017397\n",
            "loss test: 0.14698075611843894\n",
            "accuracy test: 0.8472222222222222\n",
            "loss train: 0.06982155610439647\n",
            "accuracy train: 0.9805149617258176\n",
            "loss test: 0.14666649368630522\n",
            "accuracy test: 0.8472222222222222\n",
            "loss train: 0.06889763793109704\n",
            "accuracy train: 0.9805149617258176\n",
            "loss test: 0.14636540455004182\n",
            "accuracy test: 0.8472222222222222\n",
            "loss train: 0.06800509309056557\n",
            "accuracy train: 0.9805149617258176\n",
            "loss test: 0.146077348692769\n",
            "accuracy test: 0.8472222222222222\n",
            "loss train: 0.06713728542635841\n",
            "accuracy train: 0.9805149617258176\n",
            "loss test: 0.14579784959095246\n",
            "accuracy test: 0.8472222222222222\n",
            "loss train: 0.06628626147576293\n",
            "accuracy train: 0.9812108559498957\n",
            "loss test: 0.14552214932970742\n",
            "accuracy test: 0.8472222222222222\n",
            "loss train: 0.06545009385826521\n",
            "accuracy train: 0.9812108559498957\n",
            "loss test: 0.14524686652630764\n",
            "accuracy test: 0.8472222222222222\n",
            "loss train: 0.06463294211157855\n",
            "accuracy train: 0.9819067501739736\n",
            "loss test: 0.14496958808361604\n",
            "accuracy test: 0.8472222222222222\n",
            "loss train: 0.06383213472476025\n",
            "accuracy train: 0.9819067501739736\n",
            "loss test: 0.14468775764340738\n",
            "accuracy test: 0.8472222222222222\n",
            "loss train: 0.06304103052256879\n",
            "accuracy train: 0.9819067501739736\n",
            "loss test: 0.14439753816041678\n",
            "accuracy test: 0.85\n",
            "loss train: 0.062254630123211946\n",
            "accuracy train: 0.9819067501739736\n",
            "loss test: 0.1440949018729922\n",
            "accuracy test: 0.85\n",
            "loss train: 0.06147046708939946\n",
            "accuracy train: 0.9819067501739736\n",
            "loss test: 0.14377790084015443\n",
            "accuracy test: 0.85\n",
            "loss train: 0.06068919331990116\n",
            "accuracy train: 0.9826026443980515\n",
            "loss test: 0.14344699292814583\n",
            "accuracy test: 0.85\n",
            "loss train: 0.05991413140410635\n",
            "accuracy train: 0.9832985386221295\n",
            "loss test: 0.14310313841425848\n",
            "accuracy test: 0.85\n",
            "loss train: 0.05914744874255584\n",
            "accuracy train: 0.9839944328462074\n",
            "loss test: 0.14274618621013754\n",
            "accuracy test: 0.85\n",
            "loss train: 0.05838790042004603\n",
            "accuracy train: 0.9846903270702854\n",
            "loss test: 0.14237595408016884\n",
            "accuracy test: 0.8527777777777777\n",
            "loss train: 0.0576385725653208\n",
            "accuracy train: 0.9860821155184412\n",
            "loss test: 0.1419929950465725\n",
            "accuracy test: 0.8527777777777777\n",
            "loss train: 0.056910689153112946\n",
            "accuracy train: 0.9867780097425192\n",
            "loss test: 0.14159573730947253\n",
            "accuracy test: 0.8527777777777777\n",
            "loss train: 0.05620667565188008\n",
            "accuracy train: 0.9867780097425192\n",
            "loss test: 0.1411832225442763\n",
            "accuracy test: 0.8527777777777777\n",
            "loss train: 0.05551847043146257\n",
            "accuracy train: 0.9874739039665971\n",
            "loss test: 0.14075835343228935\n",
            "accuracy test: 0.8527777777777777\n",
            "loss train: 0.05483771707980297\n",
            "accuracy train: 0.988169798190675\n",
            "loss test: 0.14032849776281975\n",
            "accuracy test: 0.8555555555555555\n",
            "loss train: 0.054159410871036304\n",
            "accuracy train: 0.988865692414753\n",
            "loss test: 0.13990807281801362\n",
            "accuracy test: 0.8611111111111112\n",
            "loss train: 0.05348299696422403\n",
            "accuracy train: 0.9895615866388309\n",
            "loss test: 0.1395155607405442\n",
            "accuracy test: 0.8611111111111112\n",
            "loss train: 0.05280702126471152\n",
            "accuracy train: 0.9902574808629089\n",
            "loss test: 0.13916174741587595\n",
            "accuracy test: 0.8611111111111112\n",
            "loss train: 0.052143365650458406\n",
            "accuracy train: 0.9902574808629089\n",
            "loss test: 0.13884279122376383\n",
            "accuracy test: 0.8638888888888889\n",
            "loss train: 0.05151085979925171\n",
            "accuracy train: 0.9909533750869868\n",
            "loss test: 0.1385437268559721\n",
            "accuracy test: 0.8638888888888889\n",
            "loss train: 0.05090518449256671\n",
            "accuracy train: 0.9909533750869868\n",
            "loss test: 0.1382516601377311\n",
            "accuracy test: 0.8638888888888889\n",
            "loss train: 0.05032430725583301\n",
            "accuracy train: 0.9916492693110647\n",
            "loss test: 0.13796107949327244\n",
            "accuracy test: 0.8638888888888889\n",
            "loss train: 0.049769705336791814\n",
            "accuracy train: 0.9916492693110647\n",
            "loss test: 0.13766881971612885\n",
            "accuracy test: 0.8638888888888889\n",
            "loss train: 0.04923827331187273\n",
            "accuracy train: 0.9916492693110647\n",
            "loss test: 0.1373719183340438\n",
            "accuracy test: 0.8638888888888889\n",
            "loss train: 0.0487240352597945\n",
            "accuracy train: 0.9916492693110647\n",
            "loss test: 0.13706899336266123\n",
            "accuracy test: 0.8638888888888889\n",
            "loss train: 0.04822109337990706\n",
            "accuracy train: 0.9923451635351427\n",
            "loss test: 0.13676056696323585\n",
            "accuracy test: 0.8638888888888889\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "\n",
        "image_2=cv2.imread(\"/content/drive/MyDrive/handwrite-digits/handwrite-5.png\")\n",
        "image_2=cv2.cvtColor(image_2, cv2.COLOR_BGR2GRAY)\n",
        "image_2 = image_2.reshape(64, 1)\n",
        "\n",
        "x = image_2\n",
        "# forward\n",
        "# layer1\n",
        "out1 = sigmoid(x.T @ W1 + B1)\n",
        "\n",
        "#layer2\n",
        "out2 = sigmoid(out1 @ W2 + B2)\n",
        "\n",
        "#layer3\n",
        "out3 = softmax(out2 @ W3 + B3)\n",
        "y_pred = out3\n",
        "\n",
        "print(np.argmax(y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J3wTZ-PowUWb",
        "outputId": "5aace662-e5a4-4b1f-dbde-0da9d730b756"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5\n"
          ]
        }
      ]
    }
  ]
}